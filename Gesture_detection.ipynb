{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "00hI9a9TgNGW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def detect_gesture_background_subtraction(test_video_path, output_video_path):\n",
        "  \"\"\"\n",
        "  This function detects gestures in a test video using background subtraction.\n",
        "\n",
        "  Args:\n",
        "      test_video_path (str): Path to the video containing random gestures.\n",
        "      output_video_path (str): Path to save the annotated output video.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create Background Subtractor (MOG2 - Mixture of Gaussians)\n",
        "  fgbg = cv2.createBackgroundSubtractorMOG2()\n",
        "\n",
        "  cap = cv2.VideoCapture(test_video_path)\n",
        "  frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "  out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "  kernel = np.ones((5, 5), np.uint8)  # Erosion/Dilation kernel\n",
        "\n",
        "  while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    # Apply background subtraction\n",
        "    fgmask = fgbg.apply(frame)\n",
        "\n",
        "    # Erode and Dilate to remove noise\n",
        "    fgmask = cv2.erode(fgmask, kernel, iterations=1)\n",
        "    fgmask = cv2.dilate(fgmask, kernel, iterations=3)\n",
        "\n",
        "    # Find contours in the foreground mask\n",
        "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Check if any contours are detected\n",
        "    if contours:\n",
        "      # Assuming the largest contour corresponds to the hand/gesture\n",
        "      largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "      # Draw bounding rectangle around the largest contour\n",
        "      x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "      cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "      # Write \"DETECTED\" on the frame\n",
        "      text_position = (x + 5, y + 20)\n",
        "      cv2.putText(frame, 'DETECTED', text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "    # Write the annotated frame to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "  # Release resources\n",
        "  cap.release()\n",
        "  out.release()\n",
        "  cv2.destroyAllWindows()\n",
        "\n",
        "# Path configuration (Modify as needed)\n",
        "test_video_path = '/content/vecteezy_slow-motion-asian-sportswoman-wearing-black-sportswear_8777747.mov'\n",
        "output_video_path = 'output_video.avi'\n",
        "\n",
        "# Call the detect_gesture function\n",
        "detect_gesture_background_subtraction(test_video_path, output_video_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NPBuGkYRgo9I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXUWNqw3jTC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}